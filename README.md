# LLM Benchmark - Rust

A high-performance benchmarking suite for comparing LLM inference across different Burn backends and hardware targets.

## ğŸ¯ Features

- **Multiple Backends**: Compare performance across:
  - **NdArray** (CPU backend)
  - **WGPU** (GPU backend via WebGPU/Metal/Vulkan)
  - Optional: **LibTorch** (PyTorch backend)

- **Lightweight GPT Model**: 
  - Pre-configured tiny and small transformer models
  - Easy to customize model architecture
  - Based on GPT-style decoder-only architecture

- **Comprehensive Metrics**:
  - Average inference time per iteration
  - Tokens per second throughput
  - Side-by-side comparison of backends

## ğŸš€ Quick Start

### Prerequisites

- Rust (latest stable)
- For GPU support: Vulkan/Metal drivers

### Build and Run

```bash
# Build the project
cargo build --release

# Run benchmarks
cargo run --release
```

## ğŸ“Š Model Configurations

### Tiny Model (Fast)
- Vocab Size: 512
- Hidden Size: 128
- Layers: 2
- Attention Heads: 2
- Max Sequence: 64

### Small Model (Balanced)
- Vocab Size: 2048
- Hidden Size: 256
- Layers: 4
- Attention Heads: 4
- Max Sequence: 128

## ğŸ”§ Customization

### Adding Custom Model Sizes

Edit `src/model.rs` to add new configurations:

```rust
impl GptConfig {
    pub fn my_custom_model() -> Self {
        Self::new()
            .with_vocab_size(4096)
            .with_hidden_size(512)
            .with_num_layers(8)
            .with_num_heads(8)
            .with_max_seq_len(256)
            .with_intermediate_size(2048)
    }
}
```

### Adjusting Benchmark Settings

Modify the benchmark configuration in `src/main.rs`:

```rust
let bench_config = BenchmarkConfig::new(
    batch_size: 8,          // Number of sequences
    sequence_length: 64,    // Tokens per sequence
    num_iterations: 100     // Benchmark iterations
);
```

### Adding LibTorch Backend

Uncomment in `Cargo.toml`:
```toml
burn-tch = "0.14"
```

Then add to main.rs:
```rust
use burn::backend::libtorch::{LibTorch, LibTorchDevice};

let device = LibTorchDevice::Cuda(0); // or LibTorchDevice::Cpu
let model = Gpt::<LibTorch>::new(&config, &device);
```

## ğŸ“ Project Structure

```
llm-benchmark/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs          # Benchmark runner
â”‚   â”œâ”€â”€ model.rs         # GPT transformer implementation
â”‚   â””â”€â”€ benchmark.rs     # Benchmarking utilities
â”œâ”€â”€ Cargo.toml          # Dependencies
â””â”€â”€ README.md
```

## ğŸ¨ Example Output

```
ğŸš€ LLM Benchmarking Suite for Rust

================================================================================
Testing TINY model configuration
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NdArray Backend (CPU)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          Benchmark Results: NdArray (CPU)                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Model Size:                                         tiny   â•‘
â•‘ Batch Size:                                            4   â•‘
â•‘ Sequence Length:                                      32   â•‘
â•‘ Iterations:                                           50   â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ Total Time:                                        1.25s   â•‘
â•‘ Avg Time/Iter:                                    25.0ms   â•‘
â•‘ Throughput:                                  2560.00 tok/s â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ¤ Contributing

Feel free to add more backends, model architectures, or benchmark metrics!

## ğŸ“ License

MIT

## ğŸ”— Built With

- [Burn](https://github.com/tracel-ai/burn) - Deep learning framework for Rust

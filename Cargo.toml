[package]
name = "llm_benchmark"
version = "0.1.0"
edition = "2021"

[features]
default = []
# PyTorch via LibTorch C++ bindings
tch = ["dep:burn-tch"]
# Pure-Rust LLM inference via HuggingFace Candle
candle = [
    "dep:candle-core",
    "dep:candle-nn",
    "dep:candle-transformers",
    "dep:hf-hub",
    "dep:tokenizers",
]
# Metal acceleration on macOS (used by candle)
metal = ["candle-core?/metal"]
# CUDA acceleration (used by candle)
cuda = ["candle-core?/cuda"]

[dependencies]
burn = "0.14"
burn-ndarray = "0.14"
burn-wgpu = "0.14"
burn-tch = { version = "0.14", optional = true }

# Pure-Rust LLM runtime (HuggingFace Candle)
# Supports loading GGUF quantised models (TinyLlama, Phi-3, Llama-3, â€¦)
candle-core        = { version = "0.8", optional = true }
candle-nn          = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }
hf-hub             = { version = "0.3", features = ["tokio"], optional = true }
tokenizers         = { version = "0.21", optional = true, default-features = false, features = ["onig"] }

# Async runtime needed by hf-hub
tokio = { version = "1", features = ["rt-multi-thread", "macros"] }

# Utilities
serde = { version = "1.0", features = ["derive"] }
rand = "0.8"

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "llm_benchmark"
harness = false
